var documenterSearchIndex = {"docs":
[{"location":"optimizers/#Optimizers-1","page":"Optimizers","title":"Optimizers","text":"","category":"section"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"An optimizer is a routine designed to update some parameters W using a gradient ∇ according to some formula. An optimizer is always needed when solving for the steady state of a Density Matrix.","category":"page"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"The most basic type of Optimizer is the Steepest-Gradient-Descent ( also known as Stochastic Gradient Descent - SGD ), which updates the weights W_i following the gradient nabla = nabla_W E, computed as the gradient of the objective function E against the parameters W. The update formula is:","category":"page"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"W_i+1 = W_i - epsilon nabla","category":"page"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"where the only parameter of the optimizer (hyperparameter, in Machine-Learning jargon) is the step size epsilon.","category":"page"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"There exist some way to improve upon this formula, for example by including momentum and friction, obtaining the Momentum Gradient Descent.","category":"page"},{"location":"optimizers/#Types-of-Optimizers-1","page":"Optimizers","title":"Types of Optimizers","text":"","category":"section"},{"location":"optimizers/#Gradient-Descent-(GD)-1","page":"Optimizers","title":"Gradient Descent (GD)","text":"","category":"section"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"Optimisers.Descent","category":"page"},{"location":"optimizers/#NeuralQuantum.Optimisers.Descent","page":"Optimizers","title":"NeuralQuantum.Optimisers.Descent","text":"Descent(η)\n\nClassic gradient descent optimiser with learning rate η. For each parameter p and its gradient δp, this runs p -= η*δp.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/#Gradient-Descent-with-Corrected-Momentum-(NesterovGD)-1","page":"Optimizers","title":"Gradient Descent with Corrected Momentum (NesterovGD)","text":"","category":"section"},{"location":"optimizers/#","page":"Optimizers","title":"Optimizers","text":"Optimisers.Nesterov","category":"page"},{"location":"networks/#Pure-State-Networks-1","page":"Networks","title":"Pure State Networks","text":"","category":"section"},{"location":"networks/#Neural-Quantum-State-1","page":"Networks","title":"Neural Quantum State","text":"","category":"section"},{"location":"networks/#","page":"Networks","title":"Networks","text":"This is an implementation of a Neural Quantum State, a simple RBM state as proposed by Carleo et Troyer Science 2018. Please note that you can chose the activation function (the third argument, f) to be either a softplus function af_softplus or a logcosh function af_logcosh. The best performance is found by combining logcosh with a spin-like hilbert space.","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"RBM","category":"page"},{"location":"networks/#NeuralQuantum.RBM","page":"Networks","title":"NeuralQuantum.RBM","text":"RBMSplit([T=Complex{STD_REAL_PREC}], N, α, f=af_softplus, [initW, initb])\n\nConstructs a Restricted Bolzmann Machine to encode a wavefunction, with weights of type T (Defaults to ComplexF32), N input neurons, N⋅α hidden neurons. This is the Neural Quantum State (NQS) Ansatz.\n\nN must match the size of the lattice.\n\nBy default the activation function is a sigmoid. You can also use logcosh by     passing as an additional parameter af_logcosh.\n\nThe initial parameters of the neurons are initialized with a rescaled normal distribution of width 0.01 for the coupling matrix and 0.05 for the local biases. The default initializers can be overriden by specifying\n\ninitW=(dims...)->rescalednormal(T, 0.01, dims...) initb=(dims...)->rescalednormal(T, 0.05, dims...) inita=(dims...)->rescaled_normal(T, 0.01, dims...)\n\nRefs:     https://arxiv.org/abs/1606.02318\n\n\n\n\n\n","category":"type"},{"location":"networks/#Mixed-State-Networks-1","page":"Networks","title":"Mixed State Networks","text":"","category":"section"},{"location":"networks/#","page":"Networks","title":"Networks","text":"In general, given an Hilbert space mathcalH with basis vecsigmainmathcalH, a density matrix defined on this space lives in the space of the Bounded Operators mathcalB with (overcomplete) basis (sigma tildesigma ) in mathcalHotimesmathcalH.  A network is a (high-dimensional non-linear) function","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"rho(sigma tildesigma W)","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"depending on the variational parameters W, and on the entries of the density matrix labelled by (sigma tildesigma).","category":"page"},{"location":"networks/#Neural-Density-Matrix-1","page":"Networks","title":"Neural Density Matrix","text":"","category":"section"},{"location":"networks/#","page":"Networks","title":"Networks","text":"Torlai et Melko PRL 2019","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"A real-valued neural network to describe a positive-semidefinite matrix. Complex numbers are generated by duplicating the structure of the network, and using one to generate the modulus and the other to generate the phase. See the article for details.","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"NDM","category":"page"},{"location":"networks/#NeuralQuantum.NDM","page":"Networks","title":"NeuralQuantum.NDM","text":"NDM([T=STD_REAL_PREC], N, αₕ, αₐ, f=af_softplus, [initW, initb, inita])\n\nConstructs a Neural Density Matrix with numerical precision T (Defaults to Float32), N input neurons, N⋅αₕ hidden neurons and N⋅αₐ ancillary neurons. This network ensure that the density matrix is always positive definite.\n\nThe number of input neurons N must match the size of the lattice.\n\nBy default the activation function is a sigmoid. You can also use logcosh by     passing as an additional parameter af_logcosh.\n\nThe initial parameters of the neurons are initialized with a rescaled normal distribution of width 0.01 for the coupling matrix and 0.005 for the local biases. The default initializers can be overriden by specifying\n\ninitW=(dims...)->rescalednormal(T, 0.01, dims...), initb=(dims...)->rescalednormal(T, 0.005, dims...), inita=(dims...)->rescaled_normal(T, 0.005, dims...))\n\nRefs:     https://arxiv.org/abs/1801.09684     https://arxiv.org/abs/1902.10104\n\n\n\n\n\n","category":"type"},{"location":"networks/#RBM-Density-Matrix-1","page":"Networks","title":"RBM Density Matrix","text":"","category":"section"},{"location":"networks/#","page":"Networks","title":"Networks","text":"A simple state that does not preserve positivity, which is a simple RBM.","category":"page"},{"location":"networks/#","page":"Networks","title":"Networks","text":"RBMSplit","category":"page"},{"location":"networks/#NeuralQuantum.RBMSplit","page":"Networks","title":"NeuralQuantum.RBMSplit","text":"RBMSplit([T=Complex{STD_REAL_PREC}], N, α, [initW, initb, inita])\n\nConstructs a Restricted Bolzmann Machine to encode a vectorised density matrix, with weights of type T (Defaults to ComplexF32), 2N input neurons, 2N⋅α hidden neurons. This network does not ensure positive-definitness of the density matrix.\n\nN must match the size of the lattice.\n\nThe initial parameters of the neurons are initialized with a rescaled normal distribution of width 0.01 for the coupling matrix and 0.05 for the local biases. The default initializers can be overriden by specifying\n\ninitW=(dims...)->rescalednormal(T, 0.01, dims...), initb=(dims...)->rescalednormal(T, 0.05, dims...), initb=(dims...)->rescaled_normal(T, 0.01, dims...),\n\nRefs:     https://arxiv.org/abs/1902.07006\n\n\n\n\n\n","category":"type"},{"location":"basics/#Basics-1","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"basics/#Defining-the-problem-1","page":"Basics","title":"Defining the problem","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"NeuralQuantum's aim is to compute the steady state of an Open Quantum System or the ground state of an Hamiltonian system. As such, the first step must be defining the quantum system you are interested in.","category":"page"},{"location":"basics/#Hilbert-space-1","page":"Basics","title":"Hilbert space","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"First, you should pick an hilbert space. As of now, only homogeneous Hilbert spaces are supported, HomogeneousFock or HomogeneousSpin If you wish, for example, to model 5 spin-1/2 particles you can create the Hilbert space as follows:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> using NeuralQuantum\n\njulia> N = 5;\njulia> hilb = HomogeneousSpin(N, 1//2)\nHilbert Space with 5 identical spins 1/2 of dimension 2\n\njulia> shape(hilb)\n5-element Array{Int64,1}:\n 2 2 2 2 2\n\njulia> state(hilb)\n5-element Array{Float32,1}:\n -1.0 -1.0 -1.0 -1.0 -1.0","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"!!! Spin vs Fock Space    You could also model the space as a Fock space with local dimension 2. This choice is formally equivalent, but in this case the states don't have values [-1.0, 1.0] but will take on the values [0.0, 1.0]. This can be useful sometimes when working with some networks. In general, the spin-space works better with logcosh activation function, while fock space works better with softplus activation function.","category":"page"},{"location":"basics/#Building-an-Hamiltonian-1","page":"Basics","title":"Building an Hamiltonian","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"To build an hamiltonian, you cannot use simple matrices. Instead, you should use our custom format that behaves similarly to a sparse matrix, but has a few additional tricks that allows us to be efficient in the kind of calculations that Variational Monte Carlo requires. To build an Hamiltonian, the simplest way is to use the standard pauli-matrices and bosonic creation/destruction operators, and compose them:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> h = 1.0; J=1.0;\njulia> H = LocalOperator(hilb)\nempty KLocalOperator on space:  HomogeneousSpin(5, 2)\n\njulia> for i=1:N\n         global H  -= h * sigmax(hilb, i)\n       end\nKLocalOperatorSum:\n   -sites: Array{Int64,1}[[1], [2], [3], [4], [5]]\n\njulia> for i=1:N\n         global H  += J * sigmaz(hilb, i) * sigmaz(hilb, mod(i, N)+1)\n       end\nKLocalOperatorSum:\n  -sites: Array{Int64,1}[[1], [2], [3], [4], [5], [1, 2], [2, 3], [3, 4], [4, 5], [1, 5]]","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"The built-in operators are sigmax, sigmay, sigmaz, sigmap,  sigmam and create, destroy. They support all the standard operations (transpose, conjugate, conjugate transpose).","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"You can also create your custom N-body operators, by specifying an hilbert space, the list of sites upon which it acts, and the matrix in the reduced space of the sites where it acts. For example, to build by yourself the sigmaz_1 * sigmaz_2 operator you can do the following:","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> sites = [1,2]\njulia> mat = diagm(0=>[1.0, -1.0, -1.0, 1.0])\njulia> KLocalOperatorRow(hilb, [1,2],  complex.(mat))\nKLocalOperator(Complex{Float64})\n  Hilb: HomogeneousSpin(5, 2)\n  sites: [1, 2]  (size: [2, 2])\n 1.0 + 0.0im   0.0 + 0.0im   0.0 + 0.0im  0.0 + 0.0im\n 0.0 + 0.0im  -1.0 + 0.0im   0.0 + 0.0im  0.0 + 0.0im\n 0.0 + 0.0im   0.0 + 0.0im  -1.0 + 0.0im  0.0 + 0.0im\n 0.0 + 0.0im   0.0 + 0.0im   0.0 + 0.0im  1.0 + 0.0im","category":"page"},{"location":"basics/#Choosing-a-Neural-Network-State-1","page":"Basics","title":"Choosing a Neural Network State","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"You should pick a state from those listed in Networks. In the following we will pick a simple restricted Boltmann Machine (RBM).","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"net  = RBM(Float32, N, 1, af_logcosh)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"In general, when working with neural networks, the first argument is an optional type for the parameters, the second is the number of sites in the system, and the others depend on the network. In the case of the RBM, the third argument is the density of the hidden layer, and the last argument is the activation function. In general we have seen that best performance is found by combining logcosh activation with spin-hilbert spaces.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"When you create a network, it has all it's weights distributed according to a gaussian with standard deviation 0.005. You can also reinitialize all weights with a gaussian distribution by using the command init_random_pars!.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> init_random_pars!(net, sigma=0.01)","category":"page"},{"location":"basics/#Chosing-a-sampler-1","page":"Basics","title":"Chosing a sampler","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"The sampler is the algorithm that selects the states in the hilbert space to be summed over. Options are Exact, which computes the whole probability distribution and samples exactly, but is very expensive and only works for relatively small (N<10) systems.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"In general, you will be using a MetropolisSampler, which uses a Metropolis-Hastings Markov Chain with a specific transition rule. Currently only a simple switching rule and an exchange rule are implemented.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"sampler = MetropolisSampler(LocalRule(), 125, N, burn=100)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"The first argument is the rule, the second argument is the length of each chain, the third argument is the number of times the LocalRule should be applied at every iteration (and should be of the order N). burn is an optional keyword argument with the number of unused iterations after the chain is resetted.","category":"page"},{"location":"basics/#Stochastic-Reconfiguration-1","page":"Basics","title":"Stochastic Reconfiguration","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"While it is possible to find the ground state with simple gradient descent, much better efficiency is achieved when the networks have less than 5000 parameters by using Natural Gradient Descent, or Stochastic Reconfiguration, which is somewhat equivalent to a second order newton method.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"The stochastic reconfiguration essentially builds a local approximation of the metric, called S-matrix, and solves the equation delta x = S^-1 nabla C where C is the cost function (the energy). This equation can be solved either by inversion or by using an iterative solver, which is much more efficient. Type ?SR in julia to see it's documentation.","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"algo  = SR(ϵ=(0.1), algorithm=sr_cg, precision=1e-3)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"Notable arguments are ϵ, which is the diagonal shift of the S matrix when inverting, the precision of the iterative solver and the algorithm used.","category":"page"},{"location":"basics/#Solving-the-problem-1","page":"Basics","title":"Solving the problem","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"All is set. You now only need to construct a BatchedSampler (which is a weird name for the object actually effecting the sampling) and optimise the weights","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"is = BatchedSampler(net, sampl, H, algo; batch_sz=8)\noptimizer = Optimisers.Descent(0.1)\n\nEvalues = Float64[];\nEerr = Float64[];\nfor i=1:300\n    ldata, prec = sample!(is)\n    ob = compute_observables(is)\n\n    push!(Evalues, real(ldata.mean))\n    push!(Eerr, ldata.error)\n    grad = precondition!(prec, algo, i)\n    Optimisers.update!(optimizer, net, grad)\nend","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"That's it. At every iteration sample!(is) will return two elements: the value of the cost function (with it's error) and an object containing data to compute the gradient, which is computed by precondition!.","category":"page"},{"location":"basics/#Computing-Observables-1","page":"Basics","title":"Computing Observables","text":"","category":"section"},{"location":"basics/#","page":"Basics","title":"Basics","text":"If you wish to compute observables, you simply need to compose the operator that represent the observable, and then add it to the BatchedSampler by doing","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"julia> Sx = LocalOperator(hilb)\njulia> for i=1:N\n          global Sx += sigmax(hilb, i)/N\n       end\n\njulia> add_observable!(is, \"Sx\", Sx)","category":"page"},{"location":"basics/#","page":"Basics","title":"Basics","text":"From now on, if you call compute_observables(is) you will obtain a dictionary with all the observables computed. Observables are computed by using the same Markov Chain used to estimate the energy (cost function) to be minimised for hamiltonian systems. In the case of Open Quantum Systems a different markov chain is used.","category":"page"},{"location":"liouvillian/#Liouvillian-Master-Equation-1","page":"Liouvillian","title":"Liouvillian Master Equation","text":"","category":"section"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"A Problem encodes a many-body quantum problem into a cost function to be minimized. Problems can encode the search for the Ground State energy of an hamiltonian of for the Steady-State of an Open Quantum System.","category":"page"},{"location":"liouvillian/#Steady-States-of-Open-Quantum-Systems-1","page":"Liouvillian","title":"Steady States of Open Quantum Systems","text":"","category":"section"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"A SteadyStateProblem encodes the steady-state of a liouvillian into the global minimum of the following cost function","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"mathcalC = langlelanglemathcalL^daggermathcalLranglerangle","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"When stochastically sampling this cost function on the hilbert space, we can evaluate it in two possible ways:","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"mathcalC = sum_sigma p(sigma) langlelanglesigma mathcalL^daggermathcalLrhoranglerangle","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"or (variance style)","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"mathcalC = sum_sigma p(sigma) langlelanglesigma mathcalLrhoranglerangle^2","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"The second version leads to smaller variance of sampled variables and also is faster to evaluate because it holds only mathcalL instead of mathcalL^daggermathcalL, as such I reccomend to use this one. To use it, set variance=true when costructing the problem.","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"SteadyStateProblem","category":"page"},{"location":"liouvillian/#Ground-State-Problems-1","page":"Liouvillian","title":"Ground State Problems","text":"","category":"section"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"A GroundStateProblem encodes the ground-state of an Hamiltonian into the global minimum of the total energy","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"mathcalC = langlepsihatHpsirangle","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"To construct a GroundStateProblem you must supply it a valid hamiltonian.","category":"page"},{"location":"liouvillian/#","page":"Liouvillian","title":"Liouvillian","text":"\nGroundStateProblem","category":"page"},{"location":"states/#State-abstract-type-1","page":"States","title":"State abstract type","text":"","category":"section"},{"location":"states/#","page":"States","title":"States","text":"State is the base abstract type which must be subtyped for the elements that span an hilbert space (or a Matrix space), and which also correspond to a configuration of the visible layer of the neural network.","category":"page"},{"location":"states/#Defining-a-new-State-concrete-struct-1","page":"States","title":"Defining a new State concrete struct","text":"","category":"section"},{"location":"states/#","page":"States","title":"States","text":"It is possible that one wants to define a new type of state, which spans the density matrix space in a different way and gives a correspondence between visible layer configurations and elements in the density matrix in a novel way. To do so, one must take care of defining the following ingredients:     - The mutable struct holding the state itself;     - Accessor methods that can be used by the sampler and the problem to compute     observables;     - Methods for modyfing the state;     - (optionally) constructors.","category":"page"},{"location":"states/#","page":"States","title":"States","text":"a State derivating from DualValuedState must define the following:","category":"page"},{"location":"states/#","page":"States","title":"States","text":"mutable struct ExampleState <: DualValuedState\n\t...\n\t...\nend","category":"page"},{"location":"states/#Constructors-1","page":"States","title":"Constructors","text":"","category":"section"},{"location":"states/#","page":"States","title":"States","text":"No default constructor must be created by itself, because it will be called from specific code of every NeuralNetwork. Though, the standard interface would require to define","category":"page"},{"location":"states/#","page":"States","title":"States","text":"ExampleState{...}({number params}, {Ints })","category":"page"},{"location":"states/#","page":"States","title":"States","text":"where number params are all parameters listing the length of the various items in the ExampleState, and {Ints} are the ints that initialize them.","category":"page"},{"location":"states/#Accessors-1","page":"States","title":"Accessors","text":"","category":"section"},{"location":"states/#","page":"States","title":"States","text":"To define a new state, all those accessors must be defined:","category":"page"},{"location":"states/#","page":"States","title":"States","text":"spacedimension(state::ExampleState) returning the size of the space where this state lives. For example, for N spins this will be 2^N\nhalf_space_dimension(state::ExampleState) returns the size of the physical hilbert space where the density matrix is defined. Usually 2^(N/2)\ntoint(state::ExampleState) returning the state expressed as an integer\nnsites(state::ExampleState) returns the number of sites where this state is defined.\nvectorindex(state::ExampleState) ????\nneurontype(state::ExampleState) returning the type of the neuron for this state\nneurontype(::Type{ExampleState}) returning the type of the neuron for this state.","category":"page"},{"location":"states/#Operations-1","page":"States","title":"Operations","text":"","category":"section"},{"location":"states/#","page":"States","title":"States","text":"flipat!(state, i::Integer)\nset!(state, i::Integer)\nadd!(State, i::Integer)\n`setzero!(state)","category":"page"},{"location":"#NeuralQuantum.jl-:-Neural-Network-states-for-Quantum-Systems-1","page":"Home","title":"NeuralQuantum.jl : Neural Network states for Quantum Systems","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"NeuralQuantum.jl is a numerical framework written in Julia to investigate Neural-Network representations of pure and mixed quantum states, and to find the Steady-State of such (Open) Quantum Systems through MonteCarlo procedures. The package can also compute the ground state of a many-body hamiltonian.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"note: Note\nThis code is currently heavily in the making. v0.2 should mark a somewhat more stable interface, but it's very different from older versions. If you find this code interesting, I'd be glad if you could let me know and give me some feedback.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Download Julia 1.3 or a more recent version (we do not support older versions of Julia). To install NeuralQuantum, run in a Julia prompt the following command.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"] add https://github.com/PhilipVinc/NeuralQuantum.jl","category":"page"},{"location":"#Basic-Usage-1","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"CurrentModule = NeuralQuantum","category":"page"},{"location":"#","page":"Home","title":"Home","text":"When using NeuralQuantum, to determine the Ground State or Steady State of a many-body problem, one needs to perform the following choices:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Chose a Neural-Network based ansatz to approximate the quantum state (see Sec. Networks);\nChose whever you want to perform a standard (stochastic) gradient descent, or if you want to use Natural Gradient Descent (also known as Stochastic Reconfiguration) (see Sec. Algorithms);\nChose the optimizer to perform the optimization, such as steepest gradient, accelerated gradient or others (see Sec. Optimizers);","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Here you can find a very short, commented example. For a more in-depth walkthrough of NeuralQuantum.jl please refer to Sec. Basics.","category":"page"},{"location":"#Table-Of-Contents-1","page":"Home","title":"Table Of Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Index-1","page":"Home","title":"Index","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"algorithms/#Algorithms-1","page":"SR","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#","page":"SR","title":"SR","text":"An algorithm specifies how the loss function is minimised.","category":"page"},{"location":"algorithms/#","page":"SR","title":"SR","text":"We support two types of algorithms: a trivial Gradient Descent and the more sophisticated Stochastic Reconfiguration method.","category":"page"},{"location":"algorithms/#","page":"SR","title":"SR","text":"Gradient\nSR","category":"page"},{"location":"algorithms/#NeuralQuantum.Gradient","page":"SR","title":"NeuralQuantum.Gradient","text":"Gradient()\n\nAlgorithm for descending along the steepest gradient with SGD-based optimizers.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NeuralQuantum.SR","page":"SR","title":"NeuralQuantum.SR","text":"SR([use_iterative=true, ϵ=0.001, λ0=100, b=0.95, λmin=1e-4, [precondition_type=sr_shift, algorithm=sr_qlp, precision=1e-4])\n\nStochastic Reconfiguration preconditioner which corrects the gradient according to the natural gradient computed as S^-1 ∇C. Using this algorithm will lead to the computation of the S matrix together with the gradient of the cost function ∇C. To compute the natural gradient S^-1∇C an iterative scheme (Minres-QLP) or a direct inversion is used.\n\nThe linear system x = S^-1 ∇C is by default solved with minres_qlp iterative solver. Alternatively you can use sr_minres, sr_cg or sr_lsq to use respectively the minres, conjugate gradient and least square solvers from IterativeSolvers.jl. For small systems you can also solve it by computing the pseudo-inverse (sr_diag), the cholesky factorisation (sr_cholesky) the pivoted-cholesky factorisation (sr_pivcholesky), and using the automatic julia solver, usually involving qr decomposition (sr_div). Those non-iterative methods are all from Base.LinearAlgebra.\n\nIf use_iterative=true the inverse matrix S^-1 is not computed, and an iterative MINRES-QLP algorithm is used to compute the product S^-1*F\n\nIf precondition_type=sr_shift then a diagonal uniform shift is added to S S –> S+ϵ*identity\n\nIf precondition_type=sr_multiplicative then a diagonal multiplicative shift is added to S S –> S + max(λ0b^n,λmin)Diagonal(diag(S)) where n is the number of the iteration.\n\n\n\n\n\n","category":"type"}]
}
