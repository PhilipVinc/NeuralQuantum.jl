# Sample network and compute observables for observables stored as
# sparse matrices, therefore where we have a list of non-zero elements in the
# column.
function sample_network!(
  res::MCMCObsEvaluationCache,
  problem::ObservablesProblem,
  net::MatrixNet, Ïƒ, wholespace=false)

  Ïƒp = parent(res.Ïƒ)

  # The denominator of this state
  lnÏˆ = net(Ïƒ)
  prob = wholespace ? exp(real(lnÏˆ)) : 1.0
  res.Zave += prob
  # Iterate through all elements in row i_Ïƒ of the matrix computing
  # Since Julia has no CSR matrices, I iterate on columns of the
  # transpose matrix
  i_Ïƒ = index(Ïƒ)

  set_index!(row(Ïƒp), i_Ïƒ)
  set_index!(col(Ïƒp), i_Ïƒ)

  # TODO : this works only with NDM. Should generalize it to states...
  for (obs_id, Oáµ—) = enumerate(problem.ObservablesTransposed)
    O_loc = 0.0+0.0im
    for row_id = Oáµ—.colptr[i_Ïƒ]:(Oáµ—.colptr[i_Ïƒ+1]-1)
      # Find nonzero elements s by doing <i_sp|Oáµ—|i_Ïƒ>
      i_Ïƒp = Oáµ—.rowval[row_id]
      # BackConvert to int
      set_index!(col(Ïƒp), i_Ïƒp)
      # Compute the log(Ïˆ(Ïƒ)/Ïˆ(Ïƒ')), by only computing differences.
      log_ratio = net(Ïƒp) - lnÏˆ
      O_loc += (Oáµ—.nzval[row_id]) * exp(log_ratio)
    end
    res.ObsAve[obs_id] += prob * O_loc
    push!(res.ObsVals[obs_id], prob * O_loc)
  end
  return res
end

# Sample network and compute observables for a KLocalOperator representation
# of observables without using lookuptables
#
function sample_network!(
  res::MCMCObsEvaluationCache,
  problem::ObservablesProblem{B,SM},
  net::MatrixNet, Ïƒ, wholespace=false) where {B, SM<:AbsLinearOperator}

  Ïƒp = parent(res.Ïƒ)

  # The denominator of this state
  lnÏˆ = net(Ïƒ)
  prob = wholespace ? exp(real(lnÏˆ)) : 1.0
  res.Zave += prob
  # Iterate through all elements in row i_Ïƒ of the matrix computing
  # Since Julia has no CSR matrices, I iterate on columns of the
  # transpose matrix
  i_Ïƒ = index(Ïƒ)

  set_index!(row(Ïƒp), i_Ïƒ)
  set_index!(col(Ïƒp), i_Ïƒ)

  # TODO : this works only with NDM. Should generalize it to states...
  for (obs_id, O) = enumerate(problem.ObservablesTransposed)
    O_loc = 0.0+0.0im
    #diffs_O = row_valdiff(O, row(Ïƒ.parent))
    for op=operators(O)
      r=local_index(row(Ïƒ.parent), sites(op))
      for (mel, changes)=op.op_conns[r]
        set_index!(col(Ïƒp), i_Ïƒ)
        for (site, val)=changes
          setat!(Ïƒp, site, val)
        end
        # Compute the log(Ïˆ(Ïƒ)/Ïˆ(Ïƒ')), by only computing differences.
        log_ratio = net(Ïƒp) - lnÏˆ
        O_loc += conj(mel) * conj(exp(log_ratio)) # maybe a conj
      end
    end

    res.ObsAve[obs_id] += prob * O_loc
    push!(res.ObsVals[obs_id], prob * O_loc)
  end
  return res
end

function sample_network!(
  res::MCMCObsEvaluationCache,
  problem::ObservablesProblem{B,SM},
  net::MatrixNet, Ïƒ::LUState, wholespace=false) where {B, SM<:AbsLinearOperator}

  # The denominator of this state
  prob = wholespace ? exp(real(net(Ïƒ))) : 1.0
  res.Zave += prob
  #i_Ïƒ = index(Ïƒ)
  Ïƒs = state(Ïƒ)
  no_changes = changes(row(ðs))

  for (obs_id, O) = enumerate(problem.ObservablesTransposed)
    O_loc = 0.0+0.0im
    diffs_O = row_valdiff(O, row(Ïƒs))
    for (mel, changes)=diffs_O
      # Compute the log(Ïˆ(Ïƒ)/Ïˆ(Ïƒ')), by only computing differences.
      log_ratio = Î”_logÏˆ(net, Ïƒ, changes, no_changes)
      O_loc += mel * exp(log_ratio)
    end

    res.ObsAve[obs_id] += prob * O_loc
    push!(res.ObsVals[obs_id], prob * O_loc)
  end
  return res
end
